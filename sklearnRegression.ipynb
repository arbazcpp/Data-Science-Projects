{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8e270c-ac30-46b3-90e4-6b31e7cf9068",
   "metadata": {},
   "source": [
    "# Creating ML Algorithms Utilizing Logistic, KNN-means, and Random-Forest Regression Methods\n",
    "## Arbaz Khan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "da506342-1bbf-4b0e-905c-4fe282491faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_csv(\"Loan_Train.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Get object\n",
    "col = df.columns[df.dtypes=='object']\n",
    "\n",
    "col = col.delete(7)\n",
    "\n",
    "df2 = pd.get_dummies(df, columns = col) \n",
    "\n",
    "# Split into train and test data on SalePrice\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df2['Loan_Status']\n",
    "x = df2.drop('Loan_Status', axis=1)\n",
    "x = x.fillna(0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ecbb5eef-c1f9-4b71-8187-9f11f0083265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education',\n",
       "       'Self_Employed', 'Property_Area'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "55e64288-1046-4a38-9c6d-451bdf33697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "573158aa-bdbe-41e3-86ba-7cb659a0c48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7133550488599348"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit using knn neighbors classifier with n = 5\n",
    "# Use ravel to fit dimensions of y\n",
    "knn.fit(x_train_scaled, y_train.values.ravel())\n",
    "\n",
    "accuracy_score(y_test, knn.predict(x_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "824956b2-8365-4064-9c00-ce4e1c85723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# Create a pipeline using StandardScaler and our knn classifier\n",
    "pipe = Pipeline([(\"standardizer\", standardizer), (\"knn\", knn)])\n",
    "\n",
    "# Search for best value of  n between 1-10\n",
    "search_space = [{\"knn__n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "\n",
    "target = y_train.values.ravel()\n",
    "features = x_train_scaled\n",
    "\n",
    "classifier = GridSearchCV(pipe, search_space, cv = 5, verbose = 0).fit(features, target)\n",
    "\n",
    "classifier.best_estimator_.get_params()[\"knn__n_neighbors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "61654700-3c69-4489-a97e-e34c7dd60164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7133550488599348"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy of knn with n = 8 \n",
    "knn8 = KNeighborsClassifier(n_neighbors = 8)\n",
    "knn8.fit(features, target)\n",
    "accuracy_score(y_test, knn8.predict(x_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bcd8e0f2-1df3-43fb-a190-3da1d4537995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 8)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.fit_transform(x_test)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "pipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n",
    "\n",
    "# Create a search space for knn, logistic regression, and randomforest\n",
    "# classifiers, with the use of hyperparameters\n",
    "search_space = [{\"classifier\":[knn]},\n",
    "               {\"classifier\":[LogisticRegression(max_iter = 500, solver = 'lbfgs')],\n",
    "                \"classifier__penalty\": ['l2'],\n",
    "                \"classifier__C\": np.logspace(0, 6, 10)},\n",
    "                {\"classifier\": [RandomForestClassifier()],\n",
    "                 \"classifier__n_estimators\": [8],\n",
    "                 \"classifier__max_features\":[1, 2, 3]}]\n",
    "\n",
    "target = y_train.values.ravel()\n",
    "features = x_train_scaled\n",
    "\n",
    "# Perform grid search over these regression methods and fit \n",
    "classifier = GridSearchCV(pipe, search_space, cv = 5, verbose = 0).fit(features, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "edc7911d-f892-41d8-8490-a035fcbace99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('classifier',\n",
      "                 LogisticRegression(C=21.544346900318832, max_iter=500))])\n"
     ]
    }
   ],
   "source": [
    "print(classifier.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "12b14346-7491-4146-9a11-3b653dd39c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7068403908794788"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression with an l1 penalty was found to be our best method\n",
    "logreg = LogisticRegression(C=21.544346900318832, max_iter = 500, penalty = 'l2', solver = 'lbfgs')\n",
    "\n",
    "logreg.fit(features, target)\n",
    "accuracy_score(y_test, logreg.predict(x_test_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a6ca1-3eee-4cad-8100-a0137d4cb1e1",
   "metadata": {},
   "source": [
    "From the results, we can see that utilizing logistic regression with C = 21.544 was our most accurate method, and the accuracy found was around 70.7%. With that said, earlier using the KNN means classifier with 8 neighbors resulted in the highest accuracy seen throughout, which was ~ 71.3%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
